{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [10/21], Clf Loss: 0.5790, Domain Loss: 0.1384, Total Loss: 0.7174\n",
      "Epoch [1/20], Step [20/21], Clf Loss: 0.4835, Domain Loss: 0.3741, Total Loss: 0.8576\n",
      "Epoch [1/20], Train Accuracy: 75.00%\n",
      "Epoch [2/20], Step [10/21], Clf Loss: 0.3128, Domain Loss: 0.3856, Total Loss: 0.6983\n",
      "Epoch [2/20], Step [20/21], Clf Loss: 0.1848, Domain Loss: 0.1834, Total Loss: 0.3681\n",
      "Epoch [2/20], Train Accuracy: 90.77%\n",
      "Epoch [3/20], Step [10/21], Clf Loss: 0.2300, Domain Loss: 0.3590, Total Loss: 0.5890\n",
      "Epoch [3/20], Step [20/21], Clf Loss: 0.1489, Domain Loss: 0.2110, Total Loss: 0.3600\n",
      "Epoch [3/20], Train Accuracy: 94.42%\n",
      "Epoch [4/20], Step [10/21], Clf Loss: 0.1094, Domain Loss: 0.2352, Total Loss: 0.3447\n",
      "Epoch [4/20], Step [20/21], Clf Loss: 0.1040, Domain Loss: 0.1289, Total Loss: 0.2330\n",
      "Epoch [4/20], Train Accuracy: 96.13%\n",
      "Epoch [5/20], Step [10/21], Clf Loss: 0.0756, Domain Loss: 0.2218, Total Loss: 0.2974\n",
      "Epoch [5/20], Step [20/21], Clf Loss: 0.0749, Domain Loss: 0.1212, Total Loss: 0.1961\n",
      "Epoch [5/20], Train Accuracy: 97.54%\n",
      "Epoch [6/20], Step [10/21], Clf Loss: 0.1067, Domain Loss: 0.2678, Total Loss: 0.3745\n",
      "Epoch [6/20], Step [20/21], Clf Loss: 0.0341, Domain Loss: 0.0689, Total Loss: 0.1030\n",
      "Epoch [6/20], Train Accuracy: 98.81%\n",
      "Epoch [7/20], Step [10/21], Clf Loss: 0.0383, Domain Loss: 0.1167, Total Loss: 0.1551\n",
      "Epoch [7/20], Step [20/21], Clf Loss: 0.0402, Domain Loss: 0.1439, Total Loss: 0.1840\n",
      "Epoch [7/20], Train Accuracy: 99.55%\n",
      "Epoch [8/20], Step [10/21], Clf Loss: 0.0429, Domain Loss: 0.1622, Total Loss: 0.2051\n",
      "Epoch [8/20], Step [20/21], Clf Loss: 0.0266, Domain Loss: 0.1103, Total Loss: 0.1369\n",
      "Epoch [8/20], Train Accuracy: 99.78%\n",
      "Epoch [9/20], Step [10/21], Clf Loss: 0.0186, Domain Loss: 0.0898, Total Loss: 0.1084\n",
      "Epoch [9/20], Step [20/21], Clf Loss: 0.0232, Domain Loss: 0.0749, Total Loss: 0.0980\n",
      "Epoch [9/20], Train Accuracy: 99.78%\n",
      "Epoch [10/20], Step [10/21], Clf Loss: 0.0139, Domain Loss: 0.0363, Total Loss: 0.0502\n",
      "Epoch [10/20], Step [20/21], Clf Loss: 0.0174, Domain Loss: 0.0896, Total Loss: 0.1070\n",
      "Epoch [10/20], Train Accuracy: 99.85%\n",
      "Epoch [11/20], Step [10/21], Clf Loss: 0.0165, Domain Loss: 0.0450, Total Loss: 0.0615\n",
      "Epoch [11/20], Step [20/21], Clf Loss: 0.0093, Domain Loss: 0.0408, Total Loss: 0.0501\n",
      "Epoch [11/20], Train Accuracy: 99.78%\n",
      "Epoch [12/20], Step [10/21], Clf Loss: 0.0213, Domain Loss: 0.0313, Total Loss: 0.0526\n",
      "Epoch [12/20], Step [20/21], Clf Loss: 0.0107, Domain Loss: 0.0489, Total Loss: 0.0597\n",
      "Epoch [12/20], Train Accuracy: 99.85%\n",
      "Epoch [13/20], Step [10/21], Clf Loss: 0.0080, Domain Loss: 0.0481, Total Loss: 0.0561\n",
      "Epoch [13/20], Step [20/21], Clf Loss: 0.0111, Domain Loss: 0.0226, Total Loss: 0.0337\n",
      "Epoch [13/20], Train Accuracy: 99.93%\n",
      "Epoch [14/20], Step [10/21], Clf Loss: 0.0044, Domain Loss: 0.0214, Total Loss: 0.0258\n",
      "Epoch [14/20], Step [20/21], Clf Loss: 0.0035, Domain Loss: 0.0217, Total Loss: 0.0252\n",
      "Epoch [14/20], Train Accuracy: 100.00%\n",
      "Epoch [15/20], Step [10/21], Clf Loss: 0.0096, Domain Loss: 0.0556, Total Loss: 0.0652\n",
      "Epoch [15/20], Step [20/21], Clf Loss: 0.0057, Domain Loss: 0.0253, Total Loss: 0.0310\n",
      "Epoch [15/20], Train Accuracy: 100.00%\n",
      "Epoch [16/20], Step [10/21], Clf Loss: 0.0032, Domain Loss: 0.0180, Total Loss: 0.0212\n",
      "Epoch [16/20], Step [20/21], Clf Loss: 0.0042, Domain Loss: 0.0192, Total Loss: 0.0235\n",
      "Epoch [16/20], Train Accuracy: 99.93%\n",
      "Epoch [17/20], Step [10/21], Clf Loss: 0.0040, Domain Loss: 0.0152, Total Loss: 0.0192\n",
      "Epoch [17/20], Step [20/21], Clf Loss: 0.0067, Domain Loss: 0.0196, Total Loss: 0.0263\n",
      "Epoch [17/20], Train Accuracy: 100.00%\n",
      "Epoch [18/20], Step [10/21], Clf Loss: 0.0053, Domain Loss: 0.0166, Total Loss: 0.0220\n",
      "Epoch [18/20], Step [20/21], Clf Loss: 0.0057, Domain Loss: 0.0153, Total Loss: 0.0210\n",
      "Epoch [18/20], Train Accuracy: 100.00%\n",
      "Epoch [19/20], Step [10/21], Clf Loss: 0.0014, Domain Loss: 0.0058, Total Loss: 0.0072\n",
      "Epoch [19/20], Step [20/21], Clf Loss: 0.0064, Domain Loss: 0.0081, Total Loss: 0.0145\n",
      "Epoch [19/20], Train Accuracy: 100.00%\n",
      "Epoch [20/20], Step [10/21], Clf Loss: 0.0039, Domain Loss: 0.0070, Total Loss: 0.0108\n",
      "Epoch [20/20], Step [20/21], Clf Loss: 0.0045, Domain Loss: 0.0069, Total Loss: 0.0113\n",
      "Epoch [20/20], Train Accuracy: 100.00%\n",
      "Epoch [20/20], Test Accuracy: 81.61%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from dataset import ImageDataset, DomainDataset\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from dann import LeNetFeatureExtractor, LabelPredictor, DomainDiscriminator, DANN\n",
    "from itertools import zip_longest\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "config={\n",
    "    \"mode\": \"train\",\n",
    "    \"batch\": 64,\n",
    "    \"epoch\": 20,\n",
    "    \"lr\": 1e-4,\n",
    "    \"cuda\": 0,\n",
    "    \"norm\": 2,\n",
    "    \"norm_type\": \"BN\",\n",
    "    \"dropout\": False,\n",
    "    \"weight_decay\": False,\n",
    "    \"opt\": \"adam\",\n",
    "    \"activation\": \"leaky_relu\",\n",
    "    \"data_augmentation\":False,\n",
    "    \"save\":False,\n",
    "    \"dropout_rate\":0.7,\n",
    "    \"alpha\": 0.1,  # You can set your desired initial alpha value here.\n",
    "    \"domain_weight\": 0.7,\n",
    "    \"source_domain_label\": 0,\n",
    "}\n",
    "\n",
    "base_dir=\"dataset/\"\n",
    "device=config[\"cuda\"]\n",
    "device=torch.device(f\"cuda:{device}\")\n",
    "\n",
    "# Define the hyperparameters\n",
    "# batch_size = 128\n",
    "# lr = 0.001\n",
    "# num_epochs = 10\n",
    "batch_size = config[\"batch\"]\n",
    "lr = config[\"lr\"]\n",
    "num_epochs = config[\"epoch\"]\n",
    "lambda_val = 0.1  # domain adversarial loss weight\n",
    "\n",
    "# Define the datasets and data loaders\n",
    "trainset = DomainDataset(base_dir + config[\"mode\"], device=device, config=config, domain=0, train=True)\n",
    "test_dataset = DomainDataset(base_dir + 'test', device=device, config=config, domain=1, train=False)\n",
    "target_domain_dataset, testdataset = random_split(test_dataset, [int(0.1 * len(test_dataset)), len(test_dataset)-int(0.1 * len(test_dataset))])\n",
    "trainloader = DataLoader(torch.utils.data.ConcatDataset([trainset, target_domain_dataset]), batch_size=config[\"batch\"], shuffle=True, num_workers=16, drop_last=True)\n",
    "testloader = DataLoader(testdataset, batch_size=config[\"batch\"], shuffle=False, num_workers=16, drop_last=True)\n",
    "\n",
    "# Define the DANN model and the optimizer\n",
    "feature_extractor = LeNetFeatureExtractor(config=config)\n",
    "label_predictor = LabelPredictor(num_classes=2,config=config)\n",
    "domain_discriminator = DomainDiscriminator(config=config)\n",
    "dann = DANN(feature_extractor, label_predictor, domain_discriminator)\n",
    "\n",
    "optimizer = optim.Adam(dann.parameters(), lr=lr)\n",
    "\n",
    "# Define the loss functions\n",
    "clf_loss_fn = nn.CrossEntropyLoss()\n",
    "domain_loss_fn = nn.BCELoss()\n",
    "\n",
    "# Train the DANN model\n",
    "for epoch in range(num_epochs):\n",
    "    dann.train()\n",
    "    num_correct_train = 0\n",
    "    num_total_train = 0\n",
    "    for i, (inputs, labels, domains) in enumerate(trainloader):\n",
    "        # Set the domain labels (0 for source, 1 for target)\n",
    "        # source_domain_labels = torch.zeros(inputs.size(0))\n",
    "        # target_domain_labels = torch.ones(inputs.size(0))\n",
    "        # domain_labels = torch.cat((source_domain_labels, target_domain_labels)).unsqueeze(1)\n",
    "        domain_labels = domains.unsqueeze(1).float()\n",
    "        # print(\"domain_labels: \",domain_labels.shape,type(domain_labels))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract features and predict labels\n",
    "        features = dann.feature_extractor(inputs)\n",
    "        label_preds = dann.label_predictor(features)\n",
    "\n",
    "        # Count Acc\n",
    "        preds = F.softmax(label_preds, dim=1)\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "        # print(pred_labels.shape,pred_labels,labels, labels.shape)\n",
    "        num_correct_train += (pred_labels == labels).sum().item()\n",
    "        num_total_train += labels.size(0)\n",
    "\n",
    "        # Compute the label prediction loss\n",
    "        clf_loss = clf_loss_fn(label_preds, labels)\n",
    "\n",
    "        # Compute the domain classification loss\n",
    "        domain_preds = dann.domain_discriminator(features)\n",
    "        domain_loss = domain_loss_fn(domain_preds, domain_labels)\n",
    "\n",
    "        # Compute the total loss and update the parameters\n",
    "        total_loss = clf_loss + domain_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the training statistics\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(\"Epoch [{}/{}], Step [{}/{}], Clf Loss: {:.4f}, Domain Loss: {:.4f}, Total Loss: {:.4f}\"\n",
    "                  .format(epoch+1, num_epochs, i+1, len(trainloader), clf_loss.item(), domain_loss.item(), total_loss.item()))\n",
    "        \n",
    "    print(\"Epoch [{}/{}], Train Accuracy: {:.2f}%\".format(epoch+1, num_epochs, 100 * num_correct_train / num_total_train))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "dann.eval()\n",
    "with torch.no_grad():\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for i, (inputs, labels, domains) in enumerate(testloader):\n",
    "        features = dann.feature_extractor(inputs)\n",
    "        logits = dann.label_predictor(features)\n",
    "        preds = F.softmax(logits, dim=1)\n",
    "        pred_labels = torch.argmax(preds, dim=1)\n",
    "        # print(pred_labels.shape,pred_labels,labels, labels.shape)\n",
    "        num_correct += (pred_labels == labels).sum().item()\n",
    "        num_total += labels.size(0)\n",
    "    accuracy = 100 * num_correct / num_total\n",
    "    print(\"Epoch [{}/{}], Test Accuracy: {:.2f}%\".format(epoch+1, num_epochs, accuracy))\n",
    "\n",
    "    # Set the model back to training mode\n",
    "    dann.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

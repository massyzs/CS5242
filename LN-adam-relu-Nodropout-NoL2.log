nohup: ignoring input
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/yao/Mislabel_learning/CS5242/main.py:137 in <module>                   │
│                                                                              │
│   134 │   dataset = ImageDataset(base_dir + 'test',device=device)            │
│   135 │   testloader = DataLoader(dataset, batch_size=config["batch"], shuff │
│   136 │                                                                      │
│ ❱ 137 │   train(net,trainloader,val_loader)                                  │
│   138 │   test(net,testloader)                                               │
│   139 │   writer.close()                                                     │
│   140                                                                        │
│                                                                              │
│ /home/yao/Mislabel_learning/CS5242/main.py:88 in train                       │
│                                                                              │
│    85 │   │   │   gt=gt.to(device)                                           │
│    86 │   │   │   opt.zero_grad()                                            │
│    87 │   │   │   # breakpoint()                                             │
│ ❱  88 │   │   │   y=net(img)                                                 │
│    89 │   │   │                                                              │
│    90 │   │   │   cls=torch.argmax(y,dim=1)                                  │
│    91 │   │   │   loss=criertion(y,gt)                                       │
│                                                                              │
│ /home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/m │
│ odule.py:1130 in _call_impl                                                  │
│                                                                              │
│   1127 │   │   # this function, and just call forward.                       │
│   1128 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │
│   1129 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1130 │   │   │   return forward_call(*input, **kwargs)                     │
│   1131 │   │   # Do not call functions when jit is used                      │
│   1132 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │
│   1133 │   │   if self._backward_hooks or _global_backward_hooks:            │
│                                                                              │
│ /home/yao/Mislabel_learning/CS5242/lenet.py:122 in forward                   │
│                                                                              │
│   119 │   │   # breakpoint()                                                 │
│   120 │   │   out = self.conv1(x)                                            │
│   121 │   │   if self.config["norm"]==1:                                     │
│ ❱ 122 │   │   │   out=self.norm1(out)                                        │
│   123 │   │   out = self.activation(out)                                     │
│   124 │   │   out = F.max_pool2d(out, 2)                                     │
│   125 │   │   if self.config["dropout"]:                                     │
│                                                                              │
│ /home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/m │
│ odule.py:1130 in _call_impl                                                  │
│                                                                              │
│   1127 │   │   # this function, and just call forward.                       │
│   1128 │   │   if not (self._backward_hooks or self._forward_hooks or self._ │
│   1129 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks │
│ ❱ 1130 │   │   │   return forward_call(*input, **kwargs)                     │
│   1131 │   │   # Do not call functions when jit is used                      │
│   1132 │   │   full_backward_hooks, non_full_backward_hooks = [], []         │
│   1133 │   │   if self._backward_hooks or _global_backward_hooks:            │
│                                                                              │
│ /home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/n │
│ ormalization.py:189 in forward                                               │
│                                                                              │
│   186 │   │   │   init.zeros_(self.bias)                                     │
│   187 │                                                                      │
│   188 │   def forward(self, input: Tensor) -> Tensor:                        │
│ ❱ 189 │   │   return F.layer_norm(                                           │
│   190 │   │   │   input, self.normalized_shape, self.weight, self.bias, self │
│   191 │                                                                      │
│   192 │   def extra_repr(self) -> str:                                       │
│                                                                              │
│ /home/yao/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functiona │
│ l.py:2503 in layer_norm                                                      │
│                                                                              │
│   2500 │   │   return handle_torch_function(                                 │
│   2501 │   │   │   layer_norm, (input, weight, bias), input, normalized_shap │
│   2502 │   │   )                                                             │
│ ❱ 2503 │   return torch.layer_norm(input, normalized_shape, weight, bias, ep │
│   2504                                                                       │
│   2505                                                                       │
│   2506 def group_norm(                                                       │
╰──────────────────────────────────────────────────────────────────────────────╯
RuntimeError: Given normalized_shape=[32, 512, 512], expected input with shape 
[*, 32, 512, 512], but got input of size[128, 32, 128, 128]
